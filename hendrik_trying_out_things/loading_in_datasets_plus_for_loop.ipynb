{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FAZDXSKYJBKS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a191df7",
        "outputId": "ed761e2d-ccd7-4b64-9ece-200e38d41204"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Base path for the datasets\n",
        "base_path = \"/content/drive/MyDrive/transformers/projekt/tf_project/hendrik_trying_out_things\"\n"
      ],
      "metadata": {
        "id": "0f3ictQbJXox"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ffa959a"
      },
      "source": [
        "# Task\n",
        "Load the 'test' and 'train' splits from the 'strategyqa_dataset', 'maqa_datasets', and 'gsm8k_datasets' into pandas DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e330612"
      },
      "source": [
        "## Explore Dataset Directories\n",
        "\n",
        "### Subtask:\n",
        "Examine the file structure and types within the 'strategyqa_dataset', 'maqa_datasets', and 'gsm8k_datasets' directories to determine the best method for loading data into pandas DataFrames, specifically looking for 'test' and 'train' splits. This involves listing files and their extensions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1025cb24"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin exploring the dataset directories, I will first use the `ls -R` command to list the contents of the root directory and confirm the presence and structure of the requested dataset directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fc79637",
        "outputId": "f94c1c12-f242-4af5-eb8d-74db412cc961"
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# 1. List the contents of 'strategyqa_dataset' directory\n",
        "print(\"\\nContents of strategyqa_dataset:\")\n",
        "!ls -R {base_path}/strategyqa_dataset\n",
        "\n",
        "# 2. List the contents of 'maqa_datasets' directory\n",
        "print(\"\\nContents of maqa_datasets:\")\n",
        "!ls -R {base_path}/maqa_datasets\n",
        "\n",
        "# 3. List the contents of 'gsm8k_datasets' directory\n",
        "print(\"\\nContents of gsm8k_datasets:\")\n",
        "!ls -R {base_path}/gsm8k_datasets"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contents of strategyqa_dataset:\n",
            "/content/drive/MyDrive/transformers/projekt/tf_project/hendrik_trying_out_things/strategyqa_dataset:\n",
            "bart_decomp_dev_predictions.jsonl\n",
            "dev.json\n",
            "queries_cache.json\n",
            "strategyqa_train_paragraphs.json\n",
            "train.json\n",
            "transformer_qa_ORA-P_dev_no_placeholders.json\n",
            "transformer_qa_ORA-P_train_no_placeholders.json\n",
            "\n",
            "Contents of maqa_datasets:\n",
            "/content/drive/MyDrive/transformers/projekt/tf_project/hendrik_trying_out_things/maqa_datasets:\n",
            " MAQA_commonsense_reasoning.json\n",
            " MAQA_mathmatical_reasoning.json\n",
            " MAQA_world_knowledge_HLS.json\n",
            " MAQA_world_knowledge_nq.json\n",
            "'single_commonsens_reasoning(StrategyQA).json'\n",
            "'single_mathematical_reasoning(gsm8k).json'\n",
            "'single_world_knowledge(NQ).json'\n",
            "\n",
            "Contents of gsm8k_datasets:\n",
            "/content/drive/MyDrive/transformers/projekt/tf_project/hendrik_trying_out_things/gsm8k_datasets:\n",
            "'test-00000-of-00001 (1).parquet'  'train-00000-of-00001 (1).parquet'\n",
            " test-00000-of-00001.parquet\t    train-00000-of-00001.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b64581b"
      },
      "source": [
        "### Summary of Dataset Splits and File Types:\n",
        "\n",
        "**1. `strategyqa_dataset`:**\n",
        "*   **Test Split:** `dev.json` (JSON format)\n",
        "*   **Train Split:** `train.json` (JSON format)\n",
        "\n",
        "**2. `maqa_datasets`:**\n",
        "*   This directory contains several JSON files, but no files explicitly named 'test' or 'train'. It appears the data is organized by reasoning type (e.g., `MAQA_commonsense_reasoning.json`, `MAQA_mathmatical_reasoning.json`, `MAQA_world_knowledge_HLS.json`, `MAQA_world_knowledge_nq.json`, and other `single_...json` files). Further inspection within these files might be needed to identify the 'test' and 'train' splits if they are combined or differently named.\n",
        "\n",
        "**3. `gsm8k_datasets`:**\n",
        "*   **Test Split:** `test-00000-of-00001.parquet` (Parquet format)\n",
        "*   **Train Split:** `train-00000-of-00001.parquet` (Parquet format)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adb45dcb"
      },
      "source": [
        "## Load Data into DataFrames\n",
        "\n",
        "### Subtask:\n",
        "Load the 'test' and 'train' splits from 'strategyqa_dataset' and 'gsm8k_datasets', and all JSON files from 'maqa_datasets' into pandas DataFrames.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "590c9c6d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading specific 'test' and 'train' splits from `strategyqa_dataset` and `gsm8k_datasets` and all JSON files from `maqa_datasets` into pandas DataFrames. This involves importing pandas, setting the base path, reading JSON and Parquet files, and iterating through a directory for multiple JSON files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9f2c0c2",
        "outputId": "2f8aa048-99a6-4c43-99a4-0ce9ff4d2808"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "# 3. For strategyqa_dataset:\n",
        "strategyqa_dev_df = pd.read_json(os.path.join(base_path, 'strategyqa_dataset', 'dev.json'))\n",
        "strategyqa_train_df = pd.read_json(os.path.join(base_path, 'strategyqa_dataset', 'train.json'))\n",
        "\n",
        "# 4. For gsm8k_datasets:\n",
        "gsm8k_test_df = pd.read_parquet(os.path.join(base_path, 'gsm8k_datasets', 'test-00000-of-00001.parquet'))\n",
        "gsm8k_train_df = pd.read_parquet(os.path.join(base_path, 'gsm8k_datasets', 'train-00000-of-00001.parquet'))\n",
        "\n",
        "# 5. For maqa_datasets:\n",
        "maqa_dfs = {}\n",
        "maqa_dataset_path = os.path.join(base_path, 'maqa_datasets')\n",
        "for filename in os.listdir(maqa_dataset_path):\n",
        "    if filename.endswith('.json'):\n",
        "        file_path = os.path.join(maqa_dataset_path, filename)\n",
        "        df_name = os.path.splitext(filename)[0] # Get filename without extension\n",
        "        maqa_dfs[df_name] = pd.read_json(file_path)\n",
        "\n",
        "# 6. Print the first few rows and shape of each loaded DataFrame\n",
        "print(\"\\n--- strategyqa_dev_df ---\")\n",
        "print(strategyqa_dev_df.head())\n",
        "print(f\"Shape: {strategyqa_dev_df.shape}\")\n",
        "\n",
        "print(\"\\n--- strategyqa_train_df ---\")\n",
        "print(strategyqa_train_df.head())\n",
        "print(f\"Shape: {strategyqa_train_df.shape}\")\n",
        "\n",
        "print(\"\\n--- gsm8k_test_df ---\")\n",
        "print(gsm8k_test_df.head())\n",
        "print(f\"Shape: {gsm8k_test_df.shape}\")\n",
        "\n",
        "print(\"\\n--- gsm8k_train_df ---\")\n",
        "print(gsm8k_train_df.head())\n",
        "print(f\"Shape: {gsm8k_train_df.shape}\")\n",
        "\n",
        "print(\"\\n--- maqa_datasets DataFrames ---\")\n",
        "for name, df in maqa_dfs.items():\n",
        "    print(f\"\\nDataFrame: {name}\")\n",
        "    print(df.head())\n",
        "    print(f\"Shape: {df.shape}\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- strategyqa_dev_df ---\n",
            "                    qid                              term  \\\n",
            "0  e0044a7b4d146d611e73                   Albany, Georgia   \n",
            "1  c69397b4341b65ed080f  Saint Vincent and the Grenadines   \n",
            "2  be5c9933987f046b476e                             Greed   \n",
            "3  1932e05f10680ece229f                      Sea of Japan   \n",
            "4  fb8b656051c742f5bd27                           Lil Jon   \n",
            "\n",
            "                                         description  \\\n",
            "0                     City in Georgia, United States   \n",
            "1                           Country in the Caribbean   \n",
            "2  an inordinate or insatiable longing, especiall...   \n",
            "3       Marginal sea between Japan, Russia and Korea   \n",
            "4  American rapper, record producer and DJ from G...   \n",
            "\n",
            "                                            question  answer  \\\n",
            "0  Will the Albany in Georgia reach a hundred tho...   False   \n",
            "1  Is the language used in Saint Vincent and the ...    True   \n",
            "2  Is greed the most prevalent of the Seven Deadl...   False   \n",
            "3  Would the top of Mount Fuji stick out of the S...    True   \n",
            "4  Was Lil Jon's top ranked Billboard song a coll...   False   \n",
            "\n",
            "                                               facts  \\\n",
            "0  [Albany, GA has around 75,000 people, Albany, ...   \n",
            "1  [The primary language spoken in Saint Vincent ...   \n",
            "2  [Greed is a longing for wealth and power., Whi...   \n",
            "3  [The average depth of the Sea of Japan is  5,7...   \n",
            "4  [Lil Jon's highest ranked billboard song was Y...   \n",
            "\n",
            "                                       decomposition  \\\n",
            "0  [What is the population of Albany, Georgia?, W...   \n",
            "1  [What language is used in Saint Vincent and th...   \n",
            "2  [Is greed a  deadly sin?, Is gluttonly a deadl...   \n",
            "3  [How tall is Mount Fuji?, What is the maximum ...   \n",
            "4  [What is Lil Jon's top ranked Billboard song?,...   \n",
            "\n",
            "                                            evidence  \n",
            "0  [[[['Albany, Georgia-1']], [['Albany, New York...  \n",
            "1  [[[['Demographics of Saint Vincent and the Gre...  \n",
            "2  [[[['Seven deadly sins-1']], [['Seven deadly s...  \n",
            "3  [[[['Mount Fuji-18']], [['Sea of Japan-15']], ...  \n",
            "4  [[[['Yeah! (Usher song)-1']], [['Yeah! (Usher ...  \n",
            "Shape: (229, 8)\n",
            "\n",
            "--- strategyqa_train_df ---\n",
            "                    qid               term  \\\n",
            "0  2bc9c4f9c19c167187f2       Genghis Khan   \n",
            "1  03caf265939fab701dee         The Police   \n",
            "2  aadc8000bfcb987d6b9d  Depression (mood)   \n",
            "3  d1a138ecfa13ee277ab4          Grey seal   \n",
            "4  f945d8a4274bb3805989     Pound sterling   \n",
            "\n",
            "                                         description  \\\n",
            "0  founder and first Great Khan of the Mongol Empire   \n",
            "1                                  English rock band   \n",
            "2                      state of low mood and fatigue   \n",
            "3                                    species of seal   \n",
            "4  Official currency of the United Kingdom and ot...   \n",
            "\n",
            "                                            question  answer  \\\n",
            "0  Are more people today related to Genghis Khan ...    True   \n",
            "1  Could the members of The Police perform lawful...   False   \n",
            "2  Would a Monoamine Oxidase candy bar cheer up a...   False   \n",
            "3      Would a dog respond to bell before Grey seal?    True   \n",
            "4                      Is a pound sterling valuable?   False   \n",
            "\n",
            "                                               facts  \\\n",
            "0  [Julius Caesar had three children., Genghis Kh...   \n",
            "1  [The members of The Police were musicians, not...   \n",
            "2  [Depression is caused by low levels of seroton...   \n",
            "3  [Grey seals have no ear flaps and their ears c...   \n",
            "4  [A pound sterling is fiat money., Fiat money i...   \n",
            "\n",
            "                                       decomposition  \\\n",
            "0  [How many kids did Julius Caesar have?, How ma...   \n",
            "1  [Who can perform lawful arrests?, Are members ...   \n",
            "2  [Depression is caused by low levels of what ch...   \n",
            "3  [How sensitive is a grey seal's hearing on lan...   \n",
            "4  [What is the value of the Pound Sterling based...   \n",
            "\n",
            "                                            evidence  \n",
            "0  [[[['Caesarion-2', 'Julia (daughter of Caesar)...  \n",
            "1  [[[['Arrest-2']], [[\"Citizen's arrest-2\", 'The...  \n",
            "2  [[[['Monoamine oxidase-8']], [['Monoamine oxid...  \n",
            "3  [[[['Pinniped-24']], [['Hearing range-11', 'He...  \n",
            "4  [[[['Pound sterling-16']], [['Pound sterling-1...  \n",
            "Shape: (2061, 8)\n",
            "\n",
            "--- gsm8k_test_df ---\n",
            "                                            question  \\\n",
            "0  Janet’s ducks lay 16 eggs per day. She eats th...   \n",
            "1  A robe takes 2 bolts of blue fiber and half th...   \n",
            "2  Josh decides to try flipping a house.  He buys...   \n",
            "3  James decides to run 3 sprints 3 times a week....   \n",
            "4  Every day, Wendi feeds each of her chickens th...   \n",
            "\n",
            "                                              answer  \n",
            "0  Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eg...  \n",
            "1  It takes 2/2=<<2/2=1>>1 bolt of white fiber\\nS...  \n",
            "2  The cost of the house and repairs came out to ...  \n",
            "3  He sprints 3*3=<<3*3=9>>9 times\\nSo he runs 9*...  \n",
            "4  If each chicken eats 3 cups of feed per day, t...  \n",
            "Shape: (1319, 2)\n",
            "\n",
            "--- gsm8k_train_df ---\n",
            "                                            question  \\\n",
            "0  Natalia sold clips to 48 of her friends in Apr...   \n",
            "1  Weng earns $12 an hour for babysitting. Yester...   \n",
            "2  Betty is saving money for a new wallet which c...   \n",
            "3  Julie is reading a 120-page book. Yesterday, s...   \n",
            "4  James writes a 3-page letter to 2 different fr...   \n",
            "\n",
            "                                              answer  \n",
            "0  Natalia sold 48/2 = <<48/2=24>>24 clips in May...  \n",
            "1  Weng earns 12/60 = $<<12/60=0.2>>0.2 per minut...  \n",
            "2  In the beginning, Betty has only 100 / 2 = $<<...  \n",
            "3  Maila read 12 x 2 = <<12*2=24>>24 pages today....  \n",
            "4  He writes each friend 3*2=<<3*2=6>>6 pages a w...  \n",
            "Shape: (7473, 2)\n",
            "\n",
            "--- maqa_datasets DataFrames ---\n",
            "\n",
            "DataFrame: single_mathematical_reasoning(gsm8k)\n",
            "                                            question   answer\n",
            "0  Janet’s ducks lay 16 eggs per day. She eats th...     18.0\n",
            "1  A robe takes 2 bolts of blue fiber and half th...      3.0\n",
            "2  Josh decides to try flipping a house.  He buys...  70000.0\n",
            "3  James decides to run 3 sprints 3 times a week....    540.0\n",
            "4  Every day, Wendi feeds each of her chickens th...     20.0\n",
            "Shape: (1319, 2)\n",
            "\n",
            "DataFrame: MAQA_commonsense_reasoning\n",
            "                                            question              answer\n",
            "0  (a) Can preventive healthcare reduce STI trans...           [a, b, e]\n",
            "1  (a) Are those incapable of reproduction incapa...     [c, d, e, h, j]\n",
            "2  (a) Did Kim Il-sung network on LinkedIn?\\n(b) ...     [c, f, g, h, k]\n",
            "3  (a) Is Mozambique Drill an easy shot for Unite...  [a, b, c, f, g, j]\n",
            "4  (a) Was Aristotle a member of the House of Lor...              [c, f]\n",
            "Shape: (1000, 2)\n",
            "\n",
            "DataFrame: MAQA_mathmatical_reasoning\n",
            "                                            question        answer\n",
            "0      List all generators of the finite field Z_11.  [2, 6, 7, 8]\n",
            "1  Consider G as a group and H as its subgroup. F...  [1, 2, 4, 5]\n",
            "2  List all integer solutions x for the quadratic...       [-9, 7]\n",
            "3  List all distinct prime factors of the least c...     [2, 3, 5]\n",
            "4  List all the integer values of c that make the...   [9, 12, 15]\n",
            "Shape: (400, 2)\n",
            "\n",
            "DataFrame: MAQA_world_knowledge_nq\n",
            "                                            question  \\\n",
            "0  Who were the original members of the Traveling...   \n",
            "1  Who were the original members of the band The ...   \n",
            "2  Who were the writers of the song 'Tell Your He...   \n",
            "3  Who were the lyricists credited for the song '...   \n",
            "4  Who were the writers of the song 'Take Me to t...   \n",
            "\n",
            "                                              answer  \n",
            "0  [Bob Dylan, Tom Petty, George Harrison, Jeff L...  \n",
            "1  [Micky Dolenz, Peter Tork, [Michael Nesmith, M...  \n",
            "2       [Bernie Herms, Matthew West, Randy Phillips]  \n",
            "3  [[Michael Busbee, busbee, Michael James Ryan B...  \n",
            "4                  [Al Green, Mabon \"Teenie\" Hodges]  \n",
            "Shape: (592, 2)\n",
            "\n",
            "DataFrame: single_commonsens_reasoning(StrategyQA)\n",
            "                                            question answer\n",
            "0  Are more people today related to Genghis Khan ...   true\n",
            "1  Could the members of The Police perform lawful...  false\n",
            "2  Would a Monoamine Oxidase candy bar cheer up a...  false\n",
            "3      Would a dog respond to bell before Grey seal?   true\n",
            "4                      Is a pound sterling valuable?  false\n",
            "Shape: (2290, 2)\n",
            "\n",
            "DataFrame: MAQA_world_knowledge_HLS\n",
            "                                            question  \\\n",
            "0   What are the official languages spoken in India?   \n",
            "1  What are the full names of elements in the per...   \n",
            "2  Which countries are members of the European Un...   \n",
            "3    Which countries are considered part of the G20?   \n",
            "4    What are the major phyla in the animal kingdom?   \n",
            "\n",
            "                                              answer  \n",
            "0  [Assamese, Bengali, Bodo, Dogri, English, Guja...  \n",
            "1  [Scandium, Titanium, Vanadium, Chromium, Manga...  \n",
            "2  [Austria, Belgium, Bulgaria, Croatia, Cyprus, ...  \n",
            "3  [Argentina, Australia, Brazil, Canada, China, ...  \n",
            "4  [Porifera, Cnidaria, Platyhelminthes, Nematoda...  \n",
            "Shape: (50, 2)\n",
            "\n",
            "DataFrame: single_world_knowledge(NQ)\n",
            "                                      question  \\\n",
            "0      when did the eagles win last super bowl   \n",
            "1  who won last year's ncaa women's basketball   \n",
            "2         who was the ruler of england in 1616   \n",
            "3    what is the hot coffee mod in san andreas   \n",
            "4              who sang i ran all the way home   \n",
            "\n",
            "                                answer  \n",
            "0                               [2017]  \n",
            "1                     [South Carolina]  \n",
            "2                            [James I]  \n",
            "3  [a normally inaccessible mini-game]  \n",
            "4                        [The Impalas]  \n",
            "Shape: (1288, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98842350"
      },
      "source": [
        "## Store and Access DataFrames\n",
        "\n",
        "### Subtask:\n",
        "Organize the loaded DataFrames into a single dictionary for easy access and further processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da10d3e1"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial attempt to locate datasets under `/content/` failed, but the correct base path was identified as `/content/drive/MyDrive/transformers/projekt/tf_project/hendrik_trying_out_things/`.\n",
        "*   **`strategyqa_dataset`**:\n",
        "    *   `dev.json` (acting as test split) was loaded into `strategyqa_dev_df` with 229 rows and 8 columns (`qid`, `term`, `description`, `question`, `answer`, `facts`, `decomposition`, `evidence`).\n",
        "    *   `train.json` was loaded into `strategyqa_train_df` with 2061 rows and 8 columns.\n",
        "*   **`gsm8k_datasets`**:\n",
        "    *   `test-00000-of-00001.parquet` was loaded into `gsm8k_test_df` with 1319 rows and 2 columns (`question`, `answer`).\n",
        "    *   `train-00000-of-00001.parquet` was loaded into `gsm8k_train_df` with 7473 rows and 2 columns.\n",
        "*   **`maqa_datasets`**: This directory did not contain explicit 'test' or 'train' splits. Instead, seven individual JSON files were loaded, categorized by reasoning type, each with 2 columns:\n",
        "    *   `single_mathematical_reasoning(gsm8k)`: 1319 rows.\n",
        "    *   `MAQA_commonsense_reasoning`: 1000 rows.\n",
        "    *   `MAQA_mathmatical_reasoning`: 400 rows.\n",
        "    *   `MAQA_world_knowledge_nq`: 592 rows.\n",
        "    *   `single_commonsens_reasoning(StrategyQA)`: 2290 rows.\n",
        "    *   `MAQA_world_knowledge_HLS`: 50 rows.\n",
        "    *   `single_world_knowledge(NQ)`: 1288 rows.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Organize all loaded DataFrames into a single dictionary for centralized and efficient access for subsequent analysis steps.\n",
        "*   Given the varying column structures (e.g., `strategyqa` having 8 columns, `gsm8k` and `maqa` datasets having 2 columns), consider a standardization process or specific handling for each dataset type during combined processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Columns for strategyqa_dev_df ---\")\n",
        "print(strategyqa_dev_df.columns)\n",
        "\n",
        "print(\"\\n--- Columns for strategyqa_train_df ---\")\n",
        "print(strategyqa_train_df.columns)\n",
        "\n",
        "print(\"\\n--- Columns for gsm8k_test_df ---\")\n",
        "print(gsm8k_test_df.columns)\n",
        "\n",
        "print(\"\\n--- Columns for gsm8k_train_df ---\")\n",
        "print(gsm8k_train_df.columns)\n",
        "\n",
        "print(\"\\n--- Columns for maqa_datasets DataFrames ---\")\n",
        "for name, df in maqa_dfs.items():\n",
        "    print(f\"\\nDataFrame: {name}\")\n",
        "    print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAZVsu72Llz0",
        "outputId": "0bf95595-4b6c-41cf-9a37-a12824fd3c8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Columns for strategyqa_dev_df ---\n",
            "Index(['qid', 'term', 'description', 'question', 'answer', 'facts',\n",
            "       'decomposition', 'evidence'],\n",
            "      dtype='object')\n",
            "\n",
            "--- Columns for strategyqa_train_df ---\n",
            "Index(['qid', 'term', 'description', 'question', 'answer', 'facts',\n",
            "       'decomposition', 'evidence'],\n",
            "      dtype='object')\n",
            "\n",
            "--- Columns for gsm8k_test_df ---\n",
            "Index(['question', 'answer'], dtype='object')\n",
            "\n",
            "--- Columns for gsm8k_train_df ---\n",
            "Index(['question', 'answer'], dtype='object')\n",
            "\n",
            "--- Columns for maqa_datasets DataFrames ---\n",
            "\n",
            "DataFrame: single_mathematical_reasoning(gsm8k)\n",
            "Index(['question', 'answer'], dtype='object')\n",
            "\n",
            "DataFrame: MAQA_commonsense_reasoning\n",
            "Index(['question', 'answer'], dtype='object')\n",
            "\n",
            "DataFrame: MAQA_mathmatical_reasoning\n",
            "Index(['question', 'answer'], dtype='object')\n",
            "\n",
            "DataFrame: MAQA_world_knowledge_nq\n",
            "Index(['question', 'answer'], dtype='object')\n",
            "\n",
            "DataFrame: single_commonsens_reasoning(StrategyQA)\n",
            "Index(['question', 'answer'], dtype='object')\n",
            "\n",
            "DataFrame: MAQA_world_knowledge_HLS\n",
            "Index(['question', 'answer'], dtype='object')\n",
            "\n",
            "DataFrame: single_world_knowledge(NQ)\n",
            "Index(['question', 'answer'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0605a590",
        "outputId": "de32a2c4-ddd0-4bad-f6be-a7104835ad73"
      },
      "source": [
        "all_datasets = {\n",
        "    'strategyqa_dev': strategyqa_dev_df,\n",
        "    'strategyqa_train': strategyqa_train_df,\n",
        "    'gsm8k_test': gsm8k_test_df,\n",
        "    'gsm8k_train': gsm8k_train_df\n",
        "}\n",
        "\n",
        "# Add MAQA datasets to the main dictionary\n",
        "for name, df in maqa_dfs.items():\n",
        "    all_datasets[f'maqa_{name}'] = df\n",
        "\n",
        "print(\"All loaded DataFrames are now organized into the 'all_datasets' dictionary.\")\n",
        "print(f\"Total DataFrames in dictionary: {len(all_datasets)}\")\n",
        "print(\"Keys in all_datasets:\")\n",
        "for key in all_datasets.keys():\n",
        "    print(f\"- {key}\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All loaded DataFrames are now organized into the 'all_datasets' dictionary.\n",
            "Total DataFrames in dictionary: 11\n",
            "Keys in all_datasets:\n",
            "- strategyqa_dev\n",
            "- strategyqa_train\n",
            "- gsm8k_test\n",
            "- gsm8k_train\n",
            "- maqa_single_mathematical_reasoning(gsm8k)\n",
            "- maqa_MAQA_commonsense_reasoning\n",
            "- maqa_MAQA_mathmatical_reasoning\n",
            "- maqa_MAQA_world_knowledge_nq\n",
            "- maqa_single_commonsens_reasoning(StrategyQA)\n",
            "- maqa_MAQA_world_knowledge_HLS\n",
            "- maqa_single_world_knowledge(NQ)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "# NOTE: You would typically load your processor (tokenizer) here.\n",
        "# For demonstration, we'll create a dummy class if it's not defined.\n",
        "# If you have a specific tokenizer (e.g., from transformers library), replace this.\n",
        "if 'processor' not in locals():\n",
        "    class DummyTokenizer:\n",
        "        def decode(self, token_ids):\n",
        "            return f\"[TOKEN_{token_ids[0]}]\"\n",
        "    class DummyProcessor:\n",
        "        def __init__(self):\n",
        "            self.tokenizer = DummyTokenizer()\n",
        "    processor = DummyProcessor()\n",
        "\n",
        "def estimate_confidence_entropy(output, input_ids, debug=False):\n",
        "    entropies = []\n",
        "    for step_idx, step_scores in enumerate(output.logits):\n",
        "        # Ensure step_scores is a tensor and has at least one dimension\n",
        "        if not isinstance(step_scores, torch.Tensor):\n",
        "            step_scores = torch.tensor(step_scores)\n",
        "\n",
        "        # Assuming step_scores can be 1D (vocab_size) or 2D (batch_size, vocab_size)\n",
        "        # If it's 2D, take the first item (batch_size = 1 for a single input)\n",
        "        scores_tensor = step_scores[0] if step_scores.dim() > 1 else step_scores\n",
        "\n",
        "        probs = torch.nn.functional.softmax(scores_tensor, dim=-1)\n",
        "        topk = torch.topk(probs, 5)\n",
        "        entropy = 0.0\n",
        "        for i, (token_id, prob) in enumerate(zip(topk.indices.tolist(), topk.values.tolist())):\n",
        "            if prob > 0:\n",
        "                entropy += prob * math.log(prob + 1e-12)\n",
        "            if debug:\n",
        "                token_str = processor.tokenizer.decode([token_id])\n",
        "                print(f\"Step {step_idx}, Top {i+1}: Token {token_id} ('{token_str}'), Prob {prob:.4f}, Contribution {-prob * math.log(prob + 1e-12):.4f}\")\n",
        "        entropy = -entropy  # Make positive\n",
        "        entropies.append(entropy)\n",
        "    if not entropies:\n",
        "        return 0.0\n",
        "    avg_entropy = sum(entropies) / len(entropies)\n",
        "    return avg_entropy\n",
        "\n",
        "# Loop through each dataset and its entries\n",
        "print(\"Calculating confidence entropy for each dataset entry (simulated):\\n\")\n",
        "\n",
        "results = {}\n",
        "\n",
        "for dataset_name, df in all_datasets.items():\n",
        "    print(f\"Processing dataset: {dataset_name}\")\n",
        "    dataset_entropies = []\n",
        "\n",
        "    # The columns for question/answer vary, but most have 'question'\n",
        "    text_column = 'question' if 'question' in df.columns else None\n",
        "    # If 'question' is not available, try 'term' (for strategyqa_dev/train)\n",
        "    if text_column is None and 'term' in df.columns:\n",
        "        text_column = 'term'\n",
        "\n",
        "    if text_column is None:\n",
        "        print(f\"  Skipping dataset {dataset_name} as no suitable text column ('question' or 'term') was found.\")\n",
        "        continue\n",
        "\n",
        "    # Limiting to a few rows for demonstration to avoid long execution\n",
        "    num_rows_to_process = min(len(df), 5)\n",
        "\n",
        "    for i in range(num_rows_to_process):\n",
        "        entry_text = df.iloc[i][text_column]\n",
        "\n",
        "        # --- Placeholder for Model Inference --- #\n",
        "        # In a real scenario, you would pass `entry_text` to your transformer model\n",
        "        # to get `output` (with logits) and `input_ids`.\n",
        "        # Example (conceptual):\n",
        "        # input_ids = your_tokenizer(entry_text, return_tensors=\"pt\").input_ids\n",
        "        # model_output = your_model(input_ids)\n",
        "        # output = model_output # assuming model_output has a .logits attribute\n",
        "\n",
        "        # For demonstration, we'll create dummy output and input_ids\n",
        "        class DummyOutput:\n",
        "            def __init__(self, logits):\n",
        "                self.logits = logits\n",
        "\n",
        "        # Example dummy logits (shape: sequence_length, vocab_size)\n",
        "        # You'd replace this with actual logits from your model\n",
        "        dummy_logits = [\n",
        "            torch.randn(1, 100), # Logits for first token\n",
        "            torch.randn(1, 100), # Logits for second token\n",
        "        ]\n",
        "        dummy_output = DummyOutput(dummy_logits)\n",
        "        dummy_input_ids = torch.tensor([[101, 202, 102]]) # Example dummy input_ids\n",
        "\n",
        "        # Call the estimation function\n",
        "        confidence_entropy = estimate_confidence_entropy(dummy_output, dummy_input_ids)\n",
        "        dataset_entropies.append(confidence_entropy)\n",
        "\n",
        "        print(f\"  Row {i+1} (Text: '{entry_text[:50]}...'): Confidence Entropy = {confidence_entropy:.4f}\")\n",
        "\n",
        "    if dataset_entropies:\n",
        "        avg_dataset_entropy = sum(dataset_entropies) / len(dataset_entropies)\n",
        "        results[dataset_name] = avg_dataset_entropy\n",
        "        print(f\"  Average Confidence Entropy for {dataset_name} (processed {num_rows_to_process} rows): {avg_dataset_entropy:.4f}\\n\")\n",
        "    else:\n",
        "        print(f\"  No entropies calculated for {dataset_name}.\\n\")\n",
        "\n",
        "print(\"\\n--- Summary of Average Entropies (simulated) ---\")\n",
        "for ds_name, avg_ent in results.items():\n",
        "    print(f\"{ds_name}: {avg_ent:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6IeUPvlrHYx",
        "outputId": "a04c2524-ff95-4330-811d-5f3027fbf487"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating confidence entropy for each dataset entry (simulated):\n",
            "\n",
            "Processing dataset: strategyqa_dev\n",
            "  Row 1 (Text: 'Will the Albany in Georgia reach a hundred thousan...'): Confidence Entropy = 0.7339\n",
            "  Row 2 (Text: 'Is the language used in Saint Vincent and the Gren...'): Confidence Entropy = 0.7045\n",
            "  Row 3 (Text: 'Is greed the most prevalent of the Seven Deadly Si...'): Confidence Entropy = 0.6801\n",
            "  Row 4 (Text: 'Would the top of Mount Fuji stick out of the Sea o...'): Confidence Entropy = 0.7092\n",
            "  Row 5 (Text: 'Was Lil Jon's top ranked Billboard song a collabor...'): Confidence Entropy = 0.7065\n",
            "  Average Confidence Entropy for strategyqa_dev (processed 5 rows): 0.7068\n",
            "\n",
            "Processing dataset: strategyqa_train\n",
            "  Row 1 (Text: 'Are more people today related to Genghis Khan than...'): Confidence Entropy = 0.6906\n",
            "  Row 2 (Text: 'Could the members of The Police perform lawful arr...'): Confidence Entropy = 0.6990\n",
            "  Row 3 (Text: 'Would a Monoamine Oxidase candy bar cheer up a dep...'): Confidence Entropy = 0.7001\n",
            "  Row 4 (Text: 'Would a dog respond to bell before Grey seal?...'): Confidence Entropy = 0.7500\n",
            "  Row 5 (Text: 'Is a pound sterling valuable?...'): Confidence Entropy = 0.6918\n",
            "  Average Confidence Entropy for strategyqa_train (processed 5 rows): 0.7063\n",
            "\n",
            "Processing dataset: gsm8k_test\n",
            "  Row 1 (Text: 'Janet’s ducks lay 16 eggs per day. She eats three ...'): Confidence Entropy = 0.7351\n",
            "  Row 2 (Text: 'A robe takes 2 bolts of blue fiber and half that m...'): Confidence Entropy = 0.7644\n",
            "  Row 3 (Text: 'Josh decides to try flipping a house.  He buys a h...'): Confidence Entropy = 0.7830\n",
            "  Row 4 (Text: 'James decides to run 3 sprints 3 times a week.  He...'): Confidence Entropy = 0.6608\n",
            "  Row 5 (Text: 'Every day, Wendi feeds each of her chickens three ...'): Confidence Entropy = 0.7151\n",
            "  Average Confidence Entropy for gsm8k_test (processed 5 rows): 0.7317\n",
            "\n",
            "Processing dataset: gsm8k_train\n",
            "  Row 1 (Text: 'Natalia sold clips to 48 of her friends in April, ...'): Confidence Entropy = 0.6832\n",
            "  Row 2 (Text: 'Weng earns $12 an hour for babysitting. Yesterday,...'): Confidence Entropy = 0.7983\n",
            "  Row 3 (Text: 'Betty is saving money for a new wallet which costs...'): Confidence Entropy = 0.8201\n",
            "  Row 4 (Text: 'Julie is reading a 120-page book. Yesterday, she w...'): Confidence Entropy = 0.7434\n",
            "  Row 5 (Text: 'James writes a 3-page letter to 2 different friend...'): Confidence Entropy = 0.6715\n",
            "  Average Confidence Entropy for gsm8k_train (processed 5 rows): 0.7433\n",
            "\n",
            "Processing dataset: maqa_single_mathematical_reasoning(gsm8k)\n",
            "  Row 1 (Text: 'Janet’s ducks lay 16 eggs per day. She eats three ...'): Confidence Entropy = 0.8174\n",
            "  Row 2 (Text: 'A robe takes 2 bolts of blue fiber and half that m...'): Confidence Entropy = 0.7157\n",
            "  Row 3 (Text: 'Josh decides to try flipping a house.  He buys a h...'): Confidence Entropy = 0.7001\n",
            "  Row 4 (Text: 'James decides to run 3 sprints 3 times a week.  He...'): Confidence Entropy = 0.6865\n",
            "  Row 5 (Text: 'Every day, Wendi feeds each of her chickens three ...'): Confidence Entropy = 0.7194\n",
            "  Average Confidence Entropy for maqa_single_mathematical_reasoning(gsm8k) (processed 5 rows): 0.7278\n",
            "\n",
            "Processing dataset: maqa_MAQA_commonsense_reasoning\n",
            "  Row 1 (Text: '(a) Can preventive healthcare reduce STI transmiss...'): Confidence Entropy = 0.7302\n",
            "  Row 2 (Text: '(a) Are those incapable of reproduction incapable ...'): Confidence Entropy = 0.7991\n",
            "  Row 3 (Text: '(a) Did Kim Il-sung network on LinkedIn?\n",
            "(b) Did B...'): Confidence Entropy = 0.7304\n",
            "  Row 4 (Text: '(a) Is Mozambique Drill an easy shot for United St...'): Confidence Entropy = 0.6613\n",
            "  Row 5 (Text: '(a) Was Aristotle a member of the House of Lords?\n",
            "...'): Confidence Entropy = 0.7385\n",
            "  Average Confidence Entropy for maqa_MAQA_commonsense_reasoning (processed 5 rows): 0.7319\n",
            "\n",
            "Processing dataset: maqa_MAQA_mathmatical_reasoning\n",
            "  Row 1 (Text: 'List all generators of the finite field Z_11....'): Confidence Entropy = 0.8663\n",
            "  Row 2 (Text: 'Consider G as a group and H as its subgroup. For e...'): Confidence Entropy = 0.6320\n",
            "  Row 3 (Text: 'List all integer solutions x for the quadratic equ...'): Confidence Entropy = 0.7298\n",
            "  Row 4 (Text: 'List all distinct prime factors of the least commo...'): Confidence Entropy = 0.7252\n",
            "  Row 5 (Text: 'List all the integer values of c that make the equ...'): Confidence Entropy = 0.7810\n",
            "  Average Confidence Entropy for maqa_MAQA_mathmatical_reasoning (processed 5 rows): 0.7469\n",
            "\n",
            "Processing dataset: maqa_MAQA_world_knowledge_nq\n",
            "  Row 1 (Text: 'Who were the original members of the Traveling Wil...'): Confidence Entropy = 0.8400\n",
            "  Row 2 (Text: 'Who were the original members of the band The Monk...'): Confidence Entropy = 0.8075\n",
            "  Row 3 (Text: 'Who were the writers of the song 'Tell Your Heart ...'): Confidence Entropy = 0.6645\n",
            "  Row 4 (Text: 'Who were the lyricists credited for the song 'You ...'): Confidence Entropy = 0.7028\n",
            "  Row 5 (Text: 'Who were the writers of the song 'Take Me to the R...'): Confidence Entropy = 0.6938\n",
            "  Average Confidence Entropy for maqa_MAQA_world_knowledge_nq (processed 5 rows): 0.7417\n",
            "\n",
            "Processing dataset: maqa_single_commonsens_reasoning(StrategyQA)\n",
            "  Row 1 (Text: 'Are more people today related to Genghis Khan than...'): Confidence Entropy = 0.7127\n",
            "  Row 2 (Text: 'Could the members of The Police perform lawful arr...'): Confidence Entropy = 0.6667\n",
            "  Row 3 (Text: 'Would a Monoamine Oxidase candy bar cheer up a dep...'): Confidence Entropy = 0.7385\n",
            "  Row 4 (Text: 'Would a dog respond to bell before Grey seal?...'): Confidence Entropy = 0.7842\n",
            "  Row 5 (Text: 'Is a pound sterling valuable?...'): Confidence Entropy = 0.6646\n",
            "  Average Confidence Entropy for maqa_single_commonsens_reasoning(StrategyQA) (processed 5 rows): 0.7133\n",
            "\n",
            "Processing dataset: maqa_MAQA_world_knowledge_HLS\n",
            "  Row 1 (Text: 'What are the official languages spoken in India?...'): Confidence Entropy = 0.6519\n",
            "  Row 2 (Text: 'What are the full names of elements in the periodi...'): Confidence Entropy = 0.7350\n",
            "  Row 3 (Text: 'Which countries are members of the European Union?...'): Confidence Entropy = 0.6943\n",
            "  Row 4 (Text: 'Which countries are considered part of the G20?...'): Confidence Entropy = 0.7356\n",
            "  Row 5 (Text: 'What are the major phyla in the animal kingdom?...'): Confidence Entropy = 0.7022\n",
            "  Average Confidence Entropy for maqa_MAQA_world_knowledge_HLS (processed 5 rows): 0.7038\n",
            "\n",
            "Processing dataset: maqa_single_world_knowledge(NQ)\n",
            "  Row 1 (Text: 'when did the eagles win last super bowl...'): Confidence Entropy = 0.7631\n",
            "  Row 2 (Text: 'who won last year's ncaa women's basketball...'): Confidence Entropy = 0.8339\n",
            "  Row 3 (Text: 'who was the ruler of england in 1616...'): Confidence Entropy = 0.6935\n",
            "  Row 4 (Text: 'what is the hot coffee mod in san andreas...'): Confidence Entropy = 0.7282\n",
            "  Row 5 (Text: 'who sang i ran all the way home...'): Confidence Entropy = 0.6575\n",
            "  Average Confidence Entropy for maqa_single_world_knowledge(NQ) (processed 5 rows): 0.7352\n",
            "\n",
            "\n",
            "--- Summary of Average Entropies (simulated) ---\n",
            "strategyqa_dev: 0.7068\n",
            "strategyqa_train: 0.7063\n",
            "gsm8k_test: 0.7317\n",
            "gsm8k_train: 0.7433\n",
            "maqa_single_mathematical_reasoning(gsm8k): 0.7278\n",
            "maqa_MAQA_commonsense_reasoning: 0.7319\n",
            "maqa_MAQA_mathmatical_reasoning: 0.7469\n",
            "maqa_MAQA_world_knowledge_nq: 0.7417\n",
            "maqa_single_commonsens_reasoning(StrategyQA): 0.7133\n",
            "maqa_MAQA_world_knowledge_HLS: 0.7038\n",
            "maqa_single_world_knowledge(NQ): 0.7352\n"
          ]
        }
      ]
    }
  ]
}