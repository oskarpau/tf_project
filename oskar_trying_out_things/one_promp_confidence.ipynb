{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a94cd0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oskar/miniconda3/envs/sc_ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory] Start: CPU RSS 607.59 MiB\n",
      "           GPU 0: allocated 0.00 MiB | reserved 0.00 MiB | total 7940.12 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 19021.79it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory] Post-init: CPU RSS 2867.11 MiB\n",
      "           GPU 0: allocated 4627.64 MiB | reserved 4772.00 MiB | total 7940.12 MiB\n",
      "[Init] Loaded Qwen/Qwen3-VL-4B-Instruct in 21.93 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import math\n",
    "\n",
    "# Set this according to current environment\n",
    "RUNNING_IN_COLAB = False\n",
    "\n",
    "USE_VERBAL_CONFIDENCE = False\n",
    "\n",
    "\n",
    "def _bytes_to_mib(value: int) -> float:\n",
    "    return value / (1024 ** 2)\n",
    "\n",
    "def _print_memory_snapshot(label: str) -> None:\n",
    "    process = psutil.Process(os.getpid())\n",
    "    rss_mib = _bytes_to_mib(process.memory_info().rss)\n",
    "    print(f\"[Memory] {label}: CPU RSS {rss_mib:.2f} MiB\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        for device_idx in range(torch.cuda.device_count()):\n",
    "            allocated = _bytes_to_mib(torch.cuda.memory_allocated(device_idx))\n",
    "            reserved = _bytes_to_mib(torch.cuda.memory_reserved(device_idx))\n",
    "            total = _bytes_to_mib(torch.cuda.get_device_properties(device_idx).total_memory)\n",
    "            print(\n",
    "                f\"           GPU {device_idx}: allocated {allocated:.2f} MiB | \"\n",
    "                f\"reserved {reserved:.2f} MiB | total {total:.2f} MiB\"\n",
    "            )\n",
    "\n",
    "def estimate_confidence_max_prob(output, input_ids, debug=False):\n",
    "    \"\"\"Estimate confidence from token probabilities.\"\"\"\n",
    "    probs = []\n",
    "    # Skip the prompt part of the sequence\n",
    "    generated_token_ids = output.sequences[0][input_ids.shape[1]:]\n",
    "\n",
    "    # For each generated step, extract probability of the most probable token\n",
    "    for step_scores in output.logits:\n",
    "        probs_step = torch.nn.functional.softmax(step_scores[0], dim=-1)\n",
    "        max_prob, max_idx = torch.max(probs_step, dim=-1)\n",
    "        token_str = processor.tokenizer.decode([max_idx.item()])\n",
    "        if debug:\n",
    "            print(f\"Step: Most probable token: {token_str!r}, Prob: {max_prob.item():.4f}\")\n",
    "        probs.append(max_prob.item())\n",
    "\n",
    "    if not probs:\n",
    "        return 0.0\n",
    "\n",
    "    # Average probability of most probable tokens\n",
    "    avg_prob = sum(probs) / len(probs)\n",
    "    return avg_prob\n",
    "\n",
    "def estimate_confidence_entropy(output, input_ids, debug=False):\n",
    "    entropies = []\n",
    "    for step_idx, step_scores in enumerate(output.logits):\n",
    "        scores_tensor = step_scores[0]  # shape: (vocab_size,)\n",
    "        probs = torch.nn.functional.softmax(scores_tensor, dim=-1)\n",
    "        topk = torch.topk(probs, 5)\n",
    "        entropy = 0.0\n",
    "        for i, (token_id, prob) in enumerate(zip(topk.indices.tolist(), topk.values.tolist())):\n",
    "            if prob > 0:\n",
    "                entropy += prob * math.log(prob + 1e-12)\n",
    "            if debug:\n",
    "                token_str = processor.tokenizer.decode([token_id])\n",
    "                print(f\"Step {step_idx}, Top {i+1}: Token {token_id} ('{token_str}'), Prob {prob:.4f}, Contribution {-prob * math.log(prob + 1e-12):.4f}\")\n",
    "        entropy = -entropy  # Make positive\n",
    "        entropies.append(entropy)\n",
    "    if not entropies:\n",
    "        return 0.0\n",
    "    avg_entropy = sum(entropies) / len(entropies)\n",
    "    return avg_entropy\n",
    "\n",
    "def estimate_confidence_margin(output, input_ids, debug=False):\n",
    "    margins = []\n",
    "    for step_idx, step_scores in enumerate(output.logits):\n",
    "        scores_tensor = step_scores[0]  # shape: (vocab_size,)\n",
    "        probs = torch.nn.functional.softmax(scores_tensor, dim=-1)\n",
    "        topk = torch.topk(probs, 2)\n",
    "        margin = topk.values[0].item() - topk.values[1].item()\n",
    "        if debug:\n",
    "            token1_str = processor.tokenizer.decode([topk.indices[0].item()])\n",
    "            token2_str = processor.tokenizer.decode([topk.indices[1].item()])\n",
    "            print(f\"Step {step_idx}: Top1 '{token1_str}' Prob {topk.values[0].item():.4f} - Top2 '{token2_str}' Prob {topk.values[1].item():.4f} = Margin {margin:.4f}\")\n",
    "        margins.append(margin)\n",
    "    if not margins:\n",
    "        return 0.0\n",
    "    avg_margin = sum(margins) / len(margins)\n",
    "    return avg_margin\n",
    "\n",
    "# --- Script start ---\n",
    "_print_memory_snapshot(\"Start\")\n",
    "\n",
    "model_name = \"Qwen/Qwen3-VL-4B-Instruct\"\n",
    "\n",
    "# Configure int8 quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False\n",
    ")\n",
    "\n",
    "init_start = time.perf_counter()\n",
    "\n",
    "model = None\n",
    "\n",
    "if RUNNING_IN_COLAB:\n",
    "    # USE THIS WHEN RUNNING IN COLAB\n",
    "    model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # USE THIS LOCALLY AND ON LINUX\n",
    "    # Flash Attention only works in Linux and not in Colab\n",
    "    model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "_print_memory_snapshot(\"Post-init\")\n",
    "init_time = time.perf_counter() - init_start\n",
    "print(f\"[Init] Loaded {model_name} in {init_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1927b76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence estimation based on max probabilities took 0.0011\n",
      "Confidence estimation based on entropy took 0.0031\n",
      "Confidence estimation based on margins took 0.0018\n",
      "['Next to the coffee machine.']\n",
      "[Confidence] Estimated model confidence based on average max probabilities: 99.86%\n",
      "[Confidence] Estimated model confidence based on average entropy: 0.99\n",
      "[Confidence] Estimated model confidence based on average margin: 99.76\n",
      "[Timing] Prompt 1 took 2.47 seconds | Tokens: 7 | 2.84 tok/s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "#             {\"type\": \"text\", \"text\": \"Keep your answer very short. I need you to act like a humanoid robot. There are 3 possible location for coffee beans. Next to the coffee machine, in the living room or under the bed. Which place would you start looking at?\"},\n",
    "#             {\"type\": \"text\", \"text\": \"I need you to act like a humanoid robot. There are 3 possible location for coffee beans. Next to the coffee machine, in the living room or under the bed. Which place would you start looking at? Also make a plan.\"},\n",
    "\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Keep your answer very short. I need you to act like a humanoid robot. There are 3 possible location for coffee beans. Next to the coffee machine, in the living room or under the bed. Which place would you start looking at?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "confidence_field = \"Numerical confidence: [to be filled by model]\"\n",
    "if USE_VERBAL_CONFIDENCE:\n",
    "    messages[0][\"content\"].append({\"type\": \"text\", \"text\": confidence_field})\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# Run inference with token scores\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    output_logits=True\n",
    ")\n",
    "\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, output.sequences)\n",
    "]\n",
    "\n",
    "\n",
    "# # For each generated token, print the top 5 most probable tokens and their scores\n",
    "# for step_idx, step_scores in enumerate(output.logits):\n",
    "#     scores_tensor = step_scores[0]  # shape: (vocab_size,)\n",
    "#     probs = torch.nn.functional.softmax(scores_tensor, dim=-1)\n",
    "#     topk = torch.topk(probs, 5)\n",
    "#     print(f\"Step {step_idx}: Top 5 tokens:\")\n",
    "#     for rank, (token_id, prob) in enumerate(zip(topk.indices.tolist(), topk.values.tolist()), 1):\n",
    "#         token_str = processor.tokenizer.decode([token_id])\n",
    "#         score = scores_tensor[token_id].item()\n",
    "#         print(f\"  {rank}. Token {token_id} ('{token_str}'): Score {score:.4f}, Prob {prob:.4f}\")\n",
    "\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "elapsed = time.perf_counter() - start_time\n",
    "tokens_generated = len(generated_ids_trimmed[0])\n",
    "tokens_per_second = tokens_generated / elapsed if elapsed > 0 else float(\"inf\")\n",
    "\n",
    "# --- Compute confidence ---\n",
    "start_time_conf = time.perf_counter()\n",
    "confidence_max_prob = estimate_confidence_max_prob(output, inputs.input_ids)\n",
    "elapsed_conf = time.perf_counter() - start_time_conf\n",
    "print(f\"Confidence estimation based on max probabilities took {elapsed_conf:.4f}\")\n",
    "\n",
    "start_time_conf = time.perf_counter()\n",
    "confidence_entropy = estimate_confidence_entropy(output, inputs.input_ids)\n",
    "elapsed_conf = time.perf_counter() - start_time_conf\n",
    "print(f\"Confidence estimation based on entropy took {elapsed_conf:.4f}\")\n",
    "\n",
    "start_time_conf = time.perf_counter()\n",
    "confidence_margin = estimate_confidence_margin(output, inputs.input_ids)\n",
    "elapsed_conf = time.perf_counter() - start_time_conf\n",
    "print(f\"Confidence estimation based on margins took {elapsed_conf:.4f}\")\n",
    "\n",
    "# --- Print results ---\n",
    "print(output_text)\n",
    "print(f\"[Confidence] Estimated model confidence based on average max probabilities: {confidence_max_prob * 100:.2f}%\")\n",
    "print(f\"[Confidence] Estimated model confidence based on average entropy: {confidence_entropy * 100:.2f}\")\n",
    "print(f\"[Confidence] Estimated model confidence based on average margin: {confidence_margin * 100:.2f}\")\n",
    "print(f\"[Timing] Prompt 1 took {elapsed:.2f} seconds | Tokens: {tokens_generated} | {tokens_per_second:.2f} tok/s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
